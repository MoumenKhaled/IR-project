{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34143c74-c6d4-4aca-8af0-86a1e9efef9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Recall: 0.9124547950214646\n",
      "Precision@10: 0.08029865125240754\n",
      "Mean Average Precision: 0.10545595236115267\n",
      "Mean Reciprocal Rank: 0.2666826186832111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9124547950214646,\n",
       " 0.08029865125240754,\n",
       " 0.10545595236115267,\n",
       " 0.2666826186832111)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "class EvaluationMetrics:\n",
    "    def __init__(self, true_data_path, predictions_path):\n",
    "        self.true_data_path = true_data_path\n",
    "        self.predictions_path = predictions_path\n",
    "        self.true_data = self.load_data(self.true_data_path)\n",
    "        self.predictions = self.load_predictions(self.predictions_path)\n",
    "\n",
    "    def load_data(self, filepath):\n",
    "        with open(filepath, 'r') as file:\n",
    "            return [json.loads(line)['answer_pids'] for line in file]\n",
    "        \n",
    "    def load_predictions(self, filepath):\n",
    "        with open(filepath, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return [entry['sorted_non_zero_indices'] for entry in data]\n",
    "\n",
    "    def calculate_recall(self, true_pids, pred_indices):\n",
    "        true_set = set(true_pids)\n",
    "        pred_set = set(pred_indices)\n",
    "        if len(true_set) == 0:\n",
    "            return 0\n",
    "        return len(true_set & pred_set) / len(true_set)\n",
    "\n",
    "    def calculate_precision_at_k(self, true_pids, pred_indices, k):\n",
    "        true_set = set(true_pids)\n",
    "        pred_set = set(pred_indices[:k])\n",
    "        if len(pred_set) == 0:\n",
    "            return 0\n",
    "        return len(true_set & pred_set) / k\n",
    "\n",
    "    def average_precision(self, true_pids, pred_indices):\n",
    "        relevant = 0\n",
    "        sum_precisions = 0\n",
    "        for i, pred in enumerate(pred_indices):\n",
    "            if pred in true_pids:\n",
    "                relevant += 1\n",
    "                sum_precisions += relevant / (i + 1)\n",
    "        if relevant == 0:\n",
    "            return 0\n",
    "        return sum_precisions / len(true_pids)\n",
    "\n",
    "    def calculate_reciprocal_rank(self, true_pids, pred_indices):\n",
    "        for i, pred in enumerate(pred_indices):\n",
    "            if pred in true_pids:\n",
    "                return 1 / (i + 1)\n",
    "        return 0\n",
    "\n",
    "    def calculate_metrics(self):\n",
    "        recalls = []\n",
    "        precisions_k = []\n",
    "        aps = []\n",
    "        mrr_scores = []\n",
    "\n",
    "        for true_ids, pred_ids in zip(self.true_data, self.predictions):\n",
    "            recalls.append(self.calculate_recall(true_ids, pred_ids))\n",
    "            precisions_k.append(self.calculate_precision_at_k(true_ids, pred_ids, 10))\n",
    "            aps.append(self.average_precision(true_ids, pred_ids))\n",
    "            mrr_scores.append(self.calculate_reciprocal_rank(true_ids, pred_ids))\n",
    "\n",
    "        mean_recall = sum(recalls) / len(recalls)\n",
    "        mean_precision_at_k = sum(precisions_k) / len(precisions_k)\n",
    "        mean_ap = sum(aps) / len(aps)\n",
    "        mean_mrr = sum(mrr_scores) / len(mrr_scores)\n",
    "\n",
    "        print(f\"Mean Recall: {mean_recall}\")\n",
    "        print(f\"Precision@10: {mean_precision_at_k}\")\n",
    "        print(f\"Mean Average Precision: {mean_ap}\")\n",
    "        print(f\"Mean Reciprocal Rank: {mean_mrr}\")\n",
    "\n",
    "        return mean_recall, mean_precision_at_k, mean_ap, mean_mrr\n",
    "\n",
    "import json\n",
    "\n",
    "class EvaluationMetrics:\n",
    "    def __init__(self, true_data_path, predictions_path):\n",
    "        self.true_data_path = true_data_path\n",
    "        self.predictions_path = predictions_path\n",
    "        self.true_data = self.load_data(self.true_data_path)\n",
    "        self.predictions = self.load_predictions(self.predictions_path)\n",
    "\n",
    "    def load_data(self, filepath):\n",
    "        with open(filepath, 'r') as file:\n",
    "            return [json.loads(line)['answer_pids'] for line in file]\n",
    "        \n",
    "    def load_predictions(self, filepath):\n",
    "        with open(filepath, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return [entry['sorted_non_zero_indices'] for entry in data]\n",
    "\n",
    "    def calculate_recall(self, true_pids, pred_indices):\n",
    "        true_set = set(true_pids)\n",
    "        pred_set = set(pred_indices)\n",
    "        if len(true_set) == 0:\n",
    "            return 0\n",
    "        return len(true_set & pred_set) / len(true_set)\n",
    "\n",
    "    def calculate_precision_at_k(self, true_pids, pred_indices, k):\n",
    "        true_set = set(true_pids)\n",
    "        pred_set = set(pred_indices[:k])\n",
    "        if len(pred_set) == 0:\n",
    "            return 0\n",
    "        return len(true_set & pred_set) / k\n",
    "\n",
    "    def average_precision(self, true_pids, pred_indices):\n",
    "        relevant = 0\n",
    "        sum_precisions = 0\n",
    "        for i, pred in enumerate(pred_indices):\n",
    "            if pred in true_pids:\n",
    "                relevant += 1\n",
    "                sum_precisions += relevant / (i + 1)\n",
    "        if relevant == 0:\n",
    "            return 0\n",
    "        return sum_precisions / len(true_pids)\n",
    "\n",
    "    def calculate_reciprocal_rank(self, true_pids, pred_indices):\n",
    "        for i, pred in enumerate(pred_indices):\n",
    "            if pred in true_pids:\n",
    "                return 1 / (i + 1)\n",
    "        return 0\n",
    "\n",
    "    def calculate_metrics(self):\n",
    "        recalls = []\n",
    "        precisions_k = []\n",
    "        aps = []\n",
    "        mrr_scores = []\n",
    "\n",
    "        for true_ids, pred_ids in zip(self.true_data, self.predictions):\n",
    "            recalls.append(self.calculate_recall(true_ids, pred_ids))\n",
    "            precisions_k.append(self.calculate_precision_at_k(true_ids, pred_ids, 10))\n",
    "            aps.append(self.average_precision(true_ids, pred_ids))\n",
    "            mrr_scores.append(self.calculate_reciprocal_rank(true_ids, pred_ids))\n",
    "\n",
    "        mean_recall = sum(recalls) / len(recalls)\n",
    "        mean_precision_at_k = sum(precisions_k) / len(precisions_k)\n",
    "        mean_ap = sum(aps) / len(aps)\n",
    "        mean_mrr = sum(mrr_scores) / len(mrr_scores)\n",
    "\n",
    "        print(f\"Mean Recall: {mean_recall}\")\n",
    "        print(f\"Precision@10: {mean_precision_at_k}\")\n",
    "        print(f\"Mean Average Precision: {mean_ap}\")\n",
    "        print(f\"Mean Reciprocal Rank: {mean_mrr}\")\n",
    "\n",
    "        return mean_recall, mean_precision_at_k, mean_ap, mean_mrr\n",
    "\n",
    "\n",
    "evaluation=EvaluationMetrics('lifestyle/cleaned_queries.jsonl','lifestyle/tfidf_query_results.json')\n",
    "evaluation.calculate_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc3bbb5-f8eb-4124-b32b-47273956c31f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
